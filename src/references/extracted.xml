<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking Non-Photorealistic Rendering of Portraits</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-28">2017. July 28-29, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
							<email>paul.rosin@cs.cf.ac.uk†correspondingauthor</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mould</surname></persName>
							<email>mould@scs.carleton.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><forename type="middle">Itamar</forename><surname>Berger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><forename type="middle">Ariel</forename><surname>Shamir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghuai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Winnemöller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mould</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Berger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghuai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cardi University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carleton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Interdisciplinary Center Herzliya</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country>Israel, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Lambda Labs, Inc</orgName>
								<orgName type="institution">Cardi University</orgName>
								<address>
									<country>UK, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Interdisciplinary Center Herzliya</orgName>
								<orgName type="institution">Mainz University</orgName>
								<address>
									<country>Israel, Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Nokia Technologies</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference format</orgName>
								<orgName type="institution">Adobe Systems, Inc</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmarking Non-Photorealistic Rendering of Portraits</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of NPAR&apos;17</title>
						<meeting>NPAR&apos;17 <address><addrLine>Los Angeles, CA, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">12</biblScope>
							<date type="published" when="2017-07-28">2017. July 28-29, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3092919.3092921</idno>
					<note>978-1-4503-5081-5/17/07. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2020-03-10T15:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS •Computing methodologies → Non-photorealistic rendering</term>
					<term>Image processing</term>
					<term>* KEYWORDS evaluation, non-photorealistic rendering, image stylisation, por- traits</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a set of images for helping NPR practitioners evaluate their image-based portrait stylisation algorithms. Using a standard set both facilitates comparisons with other methods and helps ensure that presented results are representative. We give two levels of di culty, each consisting of 20 images selected systematically so as to provide good coverage of several possible portrait characteristics. We applied three existing portrait-speci c stylisation algorithms, two general-purpose stylisation algorithms, and one general learning based stylisation algorithm to the rst level of the benchmark, corresponding to the type of constrained images that have o en been used in portrait-speci c work. We found that the existing methods are generally e ective on this new image set, demonstrating that level one of the benchmark is tractable; challenges remain at level two. Results revealed several advantages conferred by portrait-speci c algorithms over general-purpose algorithms: portrait-speci c algorithms can use domain-speci c information to preserve key details such as eyes and to eliminate extraneous details, and they have more scope for semantically meaningful abstraction due to the underlying face model. Finally, we provide some thoughts on systematically extending the benchmark to higher levels of di culty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Progress in science is best served when there are means to evaluate and compare theories and methods.</p><p>antitative analysis is most desirable, as this makes systematic evaluation objective, and may also enable it to be scaled up to large numbers of tests with minimal e ort. Unfortunately, in some instances, appropriate objective analysis cannot be achieved, and so researchers have to fall back on subjective evaluation. In large part, this is the situation for non-photorealistic rendering (NPR), for which it is hard to compute a score that re ects the aesthetic qualities of the rendered output. Consequently, performance evaluation and the comparison of algorithms has not been pursued within the NPR community to the same degree as in computer vision. e limited ability to evaluate objectively has perhaps led to the lack of standard benchmarks for evaluating NPR algorithms. Typically, authors use their own images, and sometimes reuse a few images from previous NPR papers. <ref type="bibr">Recently, Mould and Rosin [2016]</ref> released the rst NPR benchmark data set, named NPRgeneral, comprising 20 images selected to satisfy a number of criteria, and to cover a range of characteristics. However, it does not have much coverage of portraits. For algorithms speci cally oriented towards stylising faces, a more focussed benchmark is needed. is paper presents a benchmark image set speci cally for portrait stylisation 1 . It is systematically designed to provide a range of the characteristics present in portraits, and to control the level of di culty: it is organised into multiple levels, with each level more challenging than the one below. We present the rst two levels in this paper and show stylisations of level 1 from six existing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Portraiture in Art</head><p>Historically, portraits served many purposes. Universally, they were intended to memorialise some aspect of the person depicted. However, even from the earliest days, artists were not satis ed with producing an exact likeness of the subject. Rather, the portrait strayed from reproduction, possibly merely to a er the subject, or possibly so as to convey some essential nature of the individual portrayed. Sometimes, the subject was someone of political or economic stature, and part of the mission of the portrait would be to communicate riches or grandeur. is could be achieved by surrounding the subject with symbolic elements, such as weapons or gold; other times, allusion would play a role, as in Bronzino's depiction of the Genoese admiral Andrea Doria in the guise of Neptune. A modern equivalent would be an illustrator depicting George Lucas as a Jedi knight. e range of poses and content in historical portraiture is broad; <ref type="figure" target="#fig_0">Figure 1</ref> gives a few examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Portraiture in NPR</head><p>ere is a long history of creating portraits in art: from painting and sculpture through to photography. Likewise, portraits have also gured in NPR, from the early days, such as Brennan's <ref type="bibr" target="#b2">[Brennan 2007</ref>] work on computer-assisted caricature generation, up to recent 1 e benchmark image set can be downloaded from h p://users.cs.cf.ac.uk/Paul.Rosin/ NPRportrait as well as from h p://gigl.scs.carleton.ca/benchmark. trends such as the convolutional neural network approach <ref type="bibr" target="#b30">[Selim et al. 2016]</ref>. Even those papers that do not speci cally target faces o en provide examples of portraits in the teaser or rst gure <ref type="bibr" target="#b7">[Galea et al. 2016;</ref><ref type="bibr" target="#b9">Haeberli 1990;</ref><ref type="bibr" target="#b19">Li and Wand 2016;</ref><ref type="bibr" target="#b25">Olsen and Gooch 2011;</ref><ref type="bibr" target="#b36">Winnemöller et al. 2012]</ref>.</p><p>While general algorithms can be applied to images of faces, specialised algorithms have also appeared. In general, some knowledge of the domain can improve stylisation algorithms: methods can bene t from specialised models. Faces are an unusually important element of images, hence have received focussed a ention <ref type="bibr" target="#b1">[Berger et al. 2013;</ref><ref type="bibr" target="#b4">Colton et al. 2008;</ref><ref type="bibr" target="#b33">Wang et al. 2013;</ref><ref type="bibr" target="#b38">Zhao and Zhu 2013]</ref>. e restricted domain makes models feasible. e use of an underlying model helps x simple problems to which human viewers are especially sensitive. Small changes, such as the omission of an eye region, can cause the audience to perceive an image dramatically di erently; even very subtle changes, such as the eye's pupil being moved, can in uence audience perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Goals of this Paper</head><p>We have two main objectives in this work. First and foremost, we hope to help systematise evaluation of portrait stylisation. Researchers can show the results of applying their stylisation algorithms to the images in the benchmark, thus exercising the algorithms over a broad range of possible faces. Having a common set of faces will facilitate comparisons between di erent methods. When researchers show results from all benchmark images, readers will know that results have not been specially selected to favor the cases where the algorithm works well, or to disfavour cases where the algorithm fails.</p><p>A secondary objective for this paper is to stimulate further portrait research. While current face-speci c methods are e ective in the constrained situations for which they were designed, historical artworks show a vast range of face types, poses, and complications such that existing automated algorithms cannot cope. Similarly, even though many photographs use conventional poses, many do not; the depiction of people in photographs is enormously varied. We urge the community to investigate more robust algorithms that can deal with a broader range of input images.</p><p>We make three main contributions in this paper. First, we provide a roadmap for a multi-stage image benchmark for portrait stylisation, where the rst level contains highly constrained images of the sort now used in portrait stylisation, and later levels introduce successively more di cult and more pronounced complications. Second, we provide two sets of 20 images each for the rst two levels of the benchmark, and describe the detailed design process that led to these image sets.</p><p>ird, we apply several stylisation algorithms, both general and face-speci c, to the rst level of the benchmark and discuss our ndings. In brief, we found that the portrait-speci c algorithms gain some robustness from the domain information, but performance degrades when the input images do not adhere to the constraints assumed by the face model. <ref type="bibr" target="#b24">Mould and Rosin [2016]</ref> created an NPR benchmark data set, NPRgeneral, containing 20 images selected to include a variety of a ributes and content, such as ne detail, long gradients, mixed contrast, and human faces. As only one of nearly 20 possible elements, human faces were present in only a few images. However, because of the interest in human faces speci cally, the existing benchmark does not su ce: the community would bene t from access to a di erent benchmark set that concentrates on faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>Although NPR benchmarking is so limited, literally hundreds of publicly available benchmark data sets exist in the eld of computer vision. From the early days, test images included portrait images such as Lena, Barbara, and Elaine. Subsequently, many data sets speci cally consisting of faces were developed for benchmarking face detection and recognition algorithms. While the older data sets contain images of a few tens of subjects [Samaria and Harter 1994] the increasing focus on performance evaluation quickly led to larger data sets containing hundreds of subjects <ref type="bibr" target="#b27">[Phillips et al. 2000</ref>].</p><p>Recently, large-scale data-driven machine learning has required massive data sets for training, such as the Facebook dataset <ref type="bibr" target="#b0">[Becker and Ortiz 2013;</ref><ref type="bibr" target="#b31">Taigman et al. 2014]</ref>.</p><p>In order to construct an NPR portrait benchmark data set, images could be sourced from one or several existing computer vision data sets. However, the older data sets were collected with overly restrictive conditions. For instance, JAFFE <ref type="bibr" target="#b22">[Lyons et al. 1999</ref>] contains only Japanese females. ORL [Samaria and Harter 1994] contains predominantly Caucasian males, the faces are very tightly cropped, and the images are low resolution. Later, larger-scale e orts, such as DARPA's FERET programme <ref type="bibr" target="#b27">[Phillips et al. 2000</ref>] are also not suitable: like the ORL and JAFFE, the image capture was too standardised, using the same physical setup and location (e.g. background) during construction of the data set. Moreover, grayscale rather than colour images were captured. In contrast, more recent data sets such as Labeled Faces in the Wild <ref type="bibr" target="#b15">[Huang et al. 2007</ref>] are too unconstrained: they contain substantial variations in pose, background, lighting, and occlusion. In an e ort parallel to ours but in the opposite direction, researchers at the University of Bath developed an image set with a range of depictions of people in artwork <ref type="bibr" target="#b34">[Westlake et al. 2016]</ref>; since these images are not photographs, they are not suitable for conventional stylisation e orts. e second aspect of performance evaluation is the need to dene protocols to carry out the evaluation, and this topic has been considered within computer vision at great length. ere is a large literature; work by <ref type="bibr" target="#b11">Haralick [1994</ref><ref type="bibr">], Forstner [1996</ref>, and acker et al. <ref type="bibr">[2008]</ref> is representative. In addition, many speci c approaches to evaluation have been developed for individual tasks such as object recognition, edge detection, character recognition, line detection, etc. Unfortunately, evaluation of NPR is more problematic than for computer vision, as it is di cult to quantify the quality of a rendered image. Not only do standard image comparison measures such as PSNR or SSIM fail to capture important perceptual and aesthetic aspects of a stylised image, but NPR lacks a ground truth against which to perform comparison. Practitioners in NPR have considered these issues <ref type="bibr" target="#b16">[Isenberg 2013]</ref>, and have suggested to employ proxy measures <ref type="bibr" target="#b13">[Hertzmann 2010</ref>] in place of directly evaluating the aesthetics of the stylised image, to carry out an authorial subjective evaluation <ref type="bibr" target="#b23">[Mould 2014</ref>], or to compare the stylised image to art works and "norms" such as automation, algorithmic elegance, novelty, or "wow factor" <ref type="bibr" target="#b10">[Hall and Lehmann 2013]</ref>; none of these is totally satisfactory. Another common approach is to perform a user study, although eliciting reliable user ratings for aesthetic judgements is not trivial <ref type="bibr" target="#b23">[Mould 2014]</ref>, and developing appropriate models of aesthetic judgement of artworks remains an ongoing topic of research <ref type="bibr" target="#b18">[Leder et al. 2004;</ref><ref type="bibr" target="#b26">Palmer et al. 2013</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRINCIPLES OF IMAGE DATASET</head><p>Many current NPR portrait systems are restricted to front-on single faces with simple backgrounds and no facial occlusion. We want to ensure that the benchmark is widely applicable; if it is too di cult then it will not be used by the community. At the same time, we would like to represent a fuller spectrum of possible images; in practice, people take a vast range of photographs, and it would be good to introduce some of these complications so as to push the capabilities of algorithmic image stylisation.</p><p>Hence, we plan to organise the benchmark into multiple stages, where the levels become increasingly unconstrained and more challenging. Within each level, we intend to produce a cross-section of possible complications. However, the di culty level should not rise too quickly, or else progress may not be visible: a gradual increase in di culty means that algorithms with small di erences in robustness will have noticeably di erent ability to successfully process successive levels.</p><p>e rst level should be a ainable by existing methods.</p><p>Our principles can be summarised as follows:</p><p>• e image set should contain a range of di erent face types. Furthermore, the images should present a broad collection of complications, capturing the range of conditions and environments where people photograph faces.</p><p>• e image set should be small enough that evaluating it manually is feasible.</p><p>• As a consequence of the tension between the rst two principles, the benchmark should be organised into multiple levels of di culty. • e rst level should correspond to the sorts of photographs used by existing portrait stylisation methods.</p><p>• e gap in di culty between level n and level n + 1 should not be too great.</p><p>Our rst level is governed by typical existing practices in portrait stylisation: adult faces in strict frontal views, neutral expression, clean backgrounds, with no ornamentation or facial hair. e second level relaxes these constraints slightly, permi ing facial hair, mild facial expressions, a bit of jewellery, and more varied backgrounds. Levels 3 and 4 will relax constraints on background clu er, lighting, expressions, poses, and especially age range: they should include children and the elderly as well as adult faces.</p><p>For higher levels, complications abound. Even with straightforward poses and expressions, faces vary considerably. Textureswhether arising from wrinkles, scars, blemishes, or facial hair -are a potential source of di culty, yet can be revealing and hence cannot be neglected. Hairstyles and choice of clothing are immensely variable. Challenges arise from various forms of facial adornment, such as piercings, jewellery, or glasses. Occlusions in the image can become quite serious, with only portions of the face visible, and in extreme cases, the face may not be visible at all.</p><p>When more of the body is depicted, the degrees of freedom available for the pose rise dramatically. More scope for accoutrements becomes available: for example, an artist might be depicted holding a paintbrush, or an animal lover shown holding a parrot. In modern photography, an individual might be shown carrying out some characteristic action, or a photograph might seek to capture an intense moment, such as in a sporting competition.</p><p>Finally, greater freedom in the surrounding context can complicate stylisation. A portrait may gain some of its impact from the surroundings depicted, or some visual interest from an unusual arrangement, such as a face seen through glass and partly obscured by a re ection. Certainly, a portrait need not be restricted to a single individual; portraits of families are a contrary example, and modern sel e culture o en produces photographs of small groups of friends.</p><p>As we ascend to the highest levels of di culty, the complications can become extreme. <ref type="figure" target="#fig_1">Figure 2</ref> shows examples of quite pronounced complications, including costumes, heavy occlusions, and multiple individuals. In the case of the drummer, the full e ect of the portrait cannot be obtained by stylising only the human subject, as the context provides much of the impact. ese examples are not even the most challenging images possible, but serve to illustrate the gulf between the rst levels of the benchmark and the full range of photos people might seek to stylise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Level 1</head><p>Since level 1 should provide images that are straightforward to stylise, many restrictions are imposed. Each image should contain only a frontal, approximately upright, and unoccluded view of a single face which has a forwards gaze direction. e images must contain essentially no background objects or clu er; we also exclude other body parts, such as the hands. e backgrounds are homogeneous, but natural -they were not manually masked out. Consequently, the images are dominated by the face, which should ll most of the image and be cropped approximately at the neck so as to include minimal clothing. To simplify the task of processing and rendering the face, we exclude both facial hair and long hair that partly covers the face. Also, the image should be free from "ornaments" such as a pipe, glasses, hat, or large pieces of jewellery. Harsh or complex lighting is avoided, and only so lighting used. Given the above restrictions, as well as the other restrictions such as copyright and size, the majority of images used for this level of the benchmark will be posed portraits rather than impromptu snapshots. We have found that such portraits tend to have a limited range of expressions: either neutral or a moderate smile. We have therefore allowed all images at level 1 to have these expressions without requiring further control; variation in expression will appear in level 2.</p><p>Next, we specify the desirable variations. ere should be a roughly equal distribution of gender. Given the variety of face shapes that occur, face shape should also be systematically controlled. We se led on the following set of folk descriptions of face shapes: {round, square, oval, heart, long}, although we note that these are not strictly de ned, and this along with subtle di erences between some shapes means that the a ribution of face shape to images is only approximate. Degree of a ractiveness, our next a ribute, is also subjective. ere is a tendency in the NPR literature to use aesthetically pleasing images with a ractive and/or interesting faces. We have speci ed three levels of a ractiveness: {less, average, more}. Finally, we aim to evenly cover three di erent ethnicities {white, asian, black} and two age groups: {young adult, middle-aged adult}. Again, when selecting images, we note that determining these a ributions will be approximate and subjective. However, it is not critical that the image characteristics are precise, but rather that a reasonably uniform sampling is carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Level 2</head><p>Level 2 contains many of the restrictions enforced in level 1: each image contains a frontal, approximately upright, unoccluded view of a single face that lls most of the image, is cropped to include minimal clothing, and does not include hands or other body parts. e background should be relatively plain, but since this requirement is not as strict as for level 1, some mostly unobtrusive background content is present. e requirement for unadorned faces is also relaxed, and so some hats and jewellery are allowed. Likewise, level 1's requirement for moderate lighting is maintained, but relaxed a li le. Gaze direction is mostly forwards, but not exclusively. Ages are again restricted to adult, but are not considered as a control variable for this level.</p><p>Regarding desirable variations, like level 1 there should be a roughly equal distribution of gender. We would like to include di erent facial expressions, and it seemed reasonable to use Ekman's <ref type="bibr" target="#b5">[Ekman 1972</ref>] universal expressions: happiness, sadness, anger, disgust, fear and surprise. However, when searching for images, we found it di cult to nd su cient examples that also satis ed the other level 2 conditions. In fact, with the exception of happiness, these expressions are not common in everyday conversations, and moreover, many other expressions that do naturally occur -thoughtfulness, agreement, confusion, and boredom, among others -are not included. erefore, to expedite sourcing suitable images, we simpli ed the required range of facial expressions to the set of {neutral, positive, negative}. Furthermore, extreme versions of facial expressions should be avoided; not only could these pose a challenge for rendering, but o en cause problems for the ing of face models, o en required for NPR portraits. e nal factor to control at level 2 is to include varieties of facial hair; we used the following categories: {none, moustache, beard, goatee, stubble}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design Matrix</head><p>Ideally, to populate the benchmark, we would like to systematically include images that provide examples of all the combinations of the portrait characteristics that we are controlling. However, a full factorial design would result in 2 × 5 × 3 × 3 × 2 = 180 images, somewhat beyond what is feasible to present in a paper and di cult to manually evaluate. us, we will sample the combinations and limit the benchmark to a target of 20 images.</p><p>When <ref type="bibr" target="#b24">Mould and Rosin [2016]</ref> created NPRgeneral, they manually selected a sample of 20 images to cover a range of desired characteristics. In this paper we take a more systematic approach, using the methodology of generating a "nearly orthogonal design matrix". is provides a representative subset of the values in the potentially high-dimensional input space of input conditions (variables) while maintaining approximate orthogonality of the input variables, which can be measured by computing the correlations between the input variables. We use the optFederov function from the R package AlgDesign <ref type="bibr" target="#b35">[Wheeler 2014</ref>], which allows a number of runs (i.e. images) to be speci ed, as well as allowing for di erent numbers of values for each of the input variables. e resulting nearly orthogonal design matrices for levels 1 and 2 are shown in <ref type="table" target="#tab_0">Tables 1 and 2.   gender face shape attractiveness ethnicity  age   female  square  average  white  young  female  round  more  white  young  male  oval  more  white  young  male  square  average  black  young  male  long  average  black  young  male  oval  less  black  young  female  heart  more  black  young  female  long  average  asian  young  male  round  less  asian  young  female  heart  less  asian  young  male  heart  average  white  middle  female  square  less  white  middle  male  long  less  white  middle  male  round  average  black  middle  female  oval  average  black  middle  female  round  less  black  middle  female  long  more  black  middle  female  oval  average  asian  middle  male  square  more  asian  middle  male</ref> heart more asian middle </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Image Selection</head><p>Images matching the characteristics in the design matrices were sourced by the authors from online photography repositories, prin- constraint that the images should be large enough so that the image height could be standardised at 1024 pixels, even a er cropping out the face.</p><p>It is preferable to obtain the images from a wide variety of photographers, rather from a single image collection as is common in computer vision face databases. We wish to ensure that a variety of cameras, lighting conditions, backgrounds and poses are included.</p><p>is will present a greater challenge to stylisation algorithms, as well as providing a more representative cross-section of images.</p><p>We note that the design matrix speci cations required the authors to estimate characteristics (face shape, a ractiveness, ethnicity, age, expression) which are not always clear from the image, and in some cases also involved subjective judgements. However, as noted before it is not critical that the image characteristics are precise, but rather that a reasonably uniform sampling is carried out.</p><p>Following this process we selected 20 images according to the design matrices. is was more di cult than expected, since the majority of Flickr photographs are taken under uncontrolled conditions, and hence have complicated backgrounds, harsh lighting, non-frontal view, occlusion or other factors that forced us to reject them. e level 1 and 2 images can be found in <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="figure" target="#fig_3">Figure 4</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NPR ALGORITHMS</head><p>We applied six stylisation methods to the benchmark set. Discussion of the outcomes is found in the following section. Here, we give an overview of each of the methods and describe the parameter se ings employed in generating the results.</p><p>Rosin and Lai's algorithm [Rosin and <ref type="bibr" target="#b28">Lai 2015]</ref> rst stylises the image with abstracted regions of at colours plus black and white lines <ref type="bibr" target="#b17">[Lai and Rosin 2014]</ref>, then ts a partial face model to the input image and a empts to detect the skin region. Shading and line rendering is stylised in the skin region, and in addition, the face model helps inform portrait-speci c enhancements: reducing line clu er; improving eye detail; colouring the lips and teeth; and inserting synthesised highlights.</p><p>Wang et al. <ref type="bibr" target="#b33">[Wang et al. 2013]</ref> proposed an example-based rendering technique that learns a non-parametric model of style by observing the geometry and tone of brush strokes in an exemplar photo-painting pair. e novelty of the approach is in modulating stroke a ributes directly rather than pixel patches (as with Image Analogies <ref type="bibr" target="#b14">[Hertzmann et al. 2001]</ref>) to render by example, and the technique is specialised to portraiture by learning the stroke models within independent semantic regions of the face. e algorithm uses a Markov Random Field (MRF) model to ensure spatial coherence of stroke style during learning and rendering.</p><p>Berger et al. <ref type="bibr" target="#b1">[Berger et al. 2013</ref>] mimic the style of speci c artists' line-drawings in a data-driven manner. Sample drawings of artists are collected and their statistics are analysed. en, given a new portrait photograph and an artist style, the algorithm rst creates a contour image by using a variant of the XDoG method <ref type="bibr" target="#b36">[Winnemöller et al. 2012</ref>]. Using the detected facial features, the face geometry is modi ed to follow the speci c artist's geometric style.</p><p>Lastly, the face contours are drawn using strokes from the artist's stroke database following the artist's drawing statistics.</p><p>Li and Wand's method <ref type="bibr" target="#b19">[Li and Wand 2016]</ref> treats styles as textures, and forces the synthesised image and the reference style image to have the same Markovian texture statistics. Non-parametric sampling is rst used to capture patches from the style image; patch matching and blending are then used to transfer the style to the synthesised image. For portrait stylisation, they include an additional content constraint that minimises the L 2 distance between the CNN encoding of the portrait photo and the synthesised image. eir method reduces implausible feature mixtures that are common to previous CNN based approaches, permi ing synthesizing photographic content with increased visual plausibility. However, the method can be too rigid for some painterly styles, generates artefacts due to mismatch between the content and the style images, and requires hundreds of rounds of iterations to achieve good results.</p><p>Winnemoeller et al. 's XDoG lter <ref type="bibr" target="#b36">[Winnemöller et al. 2012]</ref> can be conceptualised as the weighted sum of a blurred source image and a scaled di erence-of-Gaussians (DoG) response of the same image, e ectively applying unsharp masking to the DoG response. Combined with subsequent so thresholding, this computationally simple lter allows a wide range of stylistic and artistic e ects, including cartoon shading, black-and-white thresholding, and charcoal shading. Faces are handled well, despite not being treated di erently from other image content. If required, local modi cation of lter parameters, according to facial features, would be trivial to implement.</p><p>Li and Mould's stippling method <ref type="bibr" target="#b21">[Li and Mould 2011]</ref> is an adaptation of contrast-aware hal oning <ref type="bibr" target="#b20">[Li and Mould 2010]</ref>, an error di usion method where pixels are thresholded in a priority order and the resulting error distributed so as to preserve contrast: negative error goes preferentially to darker pixels, and positive error to lighter pixels. For stippling, thresholding down corresponds to placing a stipple, and thresholding up means placing nothing. e method strives to maintain the original intensity level while preserving local details. e full set of level 1 benchmark images processed by each method are shown in <ref type="figure" target="#fig_0">Figures 5 through 10</ref>. Several of the methods are capable of producing more than one output style; we show some alternative styles in <ref type="figure" target="#fig_0">Figure 11</ref>. For these gures, and indeed all images shown in the paper, we urge the reader to view the images at full resolution in electronic form: some small yet important details are not fully apparent on the page or at low resolution.</p><p>Parameter se ings were as follows:</p><p>• Painterly portraits <ref type="bibr" target="#b33">[Wang et al. 2013</ref>] used xed parameters, with the same values as were used in the original paper. • Rosin and Lai used xed parameter se ings over the benchmark, as detailed in the original paper [Rosin and <ref type="bibr" target="#b28">Lai 2015]</ref>, with an additional check: dark and light faces were di erentiated by testing whether the mean intensity of a central region in the face lies below the mid intensity range value (128). In HSI space, intensity values for three classes of pixels are normally quantised to {0,200,255}. For dark faces, the mean intensity value for each class was used instead. • Portrait sketching <ref type="bibr" target="#b1">[Berger et al. 2013</ref>] did not do any parameter tuning for these results. However, note that the results are artist-dependent, since statistics of the speci c artist are used to create the results. • e style transfer method <ref type="bibr" target="#b19">[Li and Wand 2016]</ref> used the following parameter se ings: patch size of 3 × 3 on both layer relu3 1 and relu4 1 for the style constraint. e selection of the layers is based on empirical study of the matching and blending performance of di erent layers. In general, larger patches preserve more features from the style image at the cost of being increasingly rigid; smaller patches can adapt to the content image more easily but have the risk of losing characteristic meso-structures.  thresholding operation, we used two sets of tone-mapping parameters: the values for {p, ϵ p , ϕ p } were set to {17, 69.5, 0.03} for images with predominantly dark tones, for images with lighter tones, {46.7, 79.5, 0.017}. e DoG parameters were σ e = 1.395, dog k = 1.6, σ m = 4, and 4 iterations of ETF smoothing were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>e portrait algorithms are generally successful at treating the benchmark images: they are all able to consistently stylise the images, and did not su er any major breakdowns over the benchmark's xed set of images. ough the subjects may not always be identi able as individuals, facial elements are generally clear.</p><p>General NPR algorithms, here exempli ed by XDoG and stippling, also produce recognisable images. In the case of the relatively straightforward inputs at level 1, there seems to be li le quality di erence between general methods and portrait-speci c methods. Put another way, the bene ts of the face-speci c elements of the dedicated portrait methods do not seem very strong.</p><p>However, face-speci c algorithms do have advantages. Rather than simply re-rendering the input image using low-level image processing operations, the face-speci c methods have an underlying model of the face, allowing them to make more elaboration abstractions <ref type="bibr">(Berger et al.)</ref> that would be infeasible in the absence of such a model, or improving robustness (Rosin and Lai) by xing problems that show up in the low-level processing.</p><p>Each method bene ts from the face-speci c information in a di erent way. Rosin and Lai use the ed face to add highlights and shading; the synthetic nose compensates for the faint features in image 2. Berger et al. perform shape abstraction by modifying the geometry of the face according to the artist's style and placing strokes based on a distribution obtained from hand-drawn sketches of faces. Wang et al. ensure feature preservation and are able to improve the contrast between the faces and the backgrounds. Finally, the CNNMRF method is able to propagate styles while largely preserving face structure owing to their MRF prior that encourages a layout consistent with the provided face image. However, due to the VGG model's prior training for general object detection, and the discriminative nature of eyes, there is a tendency for the CNNMRF method to include spurious eyes in the portraits.</p><p>Problems do appear in the stylised images. Rarely are signi cant details omi ed, but image elements can be distorted. e painterly rendering system has presented the nose in image 2 in a confused way. e sketchy portraits are intentionally slightly distorted, but sometimes the jaw and mouth regions are excessively changed, as in images 2, 14, and 16. Teeth, in particular, are sometimes a problem: both the sketched portraits system and the CNNMRF system ll in open mouths in an unpleasant-looking way.</p><p>e most widespread problem is the addition of spurious material, appearing in all methods to greater or lesser degree. e painterly strokes sometimes discolour the face, as in images 6 and 13. Rosin and Lai's method adds signi cant discolourations to a few images, especially images 1 and 7, and stray lines sometimes clu er the results, such as in images 12 and 19. e sketched portraits have stray lines as well, especially around the chin region; image 6 has a long erroneous stroke along the man's cheek.</p><p>e CNNMRF method has a tendency to add eyes in unlikely locations; in the Picasso style, it sometimes adds unnecessary bold lines across the face.</p><p>is litany of problems might seem to support the naive view that portrait-speci c methods do not produce obviously superior stylisations to more general low-level algorithms. However, the general methods have problems too. In the XDoG results, several face outlines are broken or weakly shown; in a few cases, such as images 9 and 18, the gaps are very large. XDoG is sensitive to the choice of threshold, and despite generally controlling for image brightness, the face areas can be patchy. Images 6 and 16 show the problem: we would like to convey the subjects' dark skin, but neither choosing a high threshold (and le ing the image become mostly white) nor choosing a lower one (and le ing the skin colour vary) are entirely satisfactory. In the stippling results, weak boundaries are not always clear, and an overall policy of greylevel preservation leads to distracting stippling coverage of the background regions.</p><p>Additional examples of general stylisation methods can be found in <ref type="figure" target="#fig_0">Figure 12</ref>. ese methods usually worked well on the benchmark images, but were occasionally less successful. For each method, we show both a typical result (above) and a awed output (below).</p><p>With its portrait-speci c aspects disabled, Rosin and Lai's method has several mild shortcomings. Faces tend to be clu ered with distracting small lines, and the eyes are o en fringed with white. In some cases (right example) the clu er from lines and colour quantisation can be severe. Gastal and Oliveira's abstraction method is successful much of the time, but important details might lack contrast and hence fade, while spurious details such as lighting changes might be enhanced; the eyes and teeth in the right-hand result provide examples. Similarly, the color shi ing of Sisley the abstract painter works well, calling to mind images from Andy Warhol; however, low-contrast details are not preserved and even high-contrast elements such as the eyes may not be presented properly in the output, depending on the vagaries of the random strokes. e variant of Hertzmann's method rarely fails badly, but small or low-contrast details may not be preserved depending on the se ings. e same comments apply to the circular scribble method: large regions of high contrast are ne, but small details and low-contrast boundaries are omi ed. <ref type="figure" target="#fig_0">Figure 13</ref> shows some results from existing portrait stylisation results applied to level 2 of the benchmark image set. e results are mixed. Since level 2 is only a small increment in di culty, output quality is not enormously worse. Nonetheless, the new complications do challenge the methods. Rosin and Lai's results su er from increased clu er, and facial expressions can prevent the synthetic lips from integrating well into the image. Wang et al.'s method fares well, nicely portraying the beard in the third image, but the face shape and expression in the second image and the facial hair in the fourth image are not entirely clear. Li and Wand's transferred Picasso style is still e ective, but the incidence of small defects is higher and the method does not seem to cope well with facial hair. In general, facial expressions and visible teeth will pose problems for methods that make more con ning assumptions about mouth pose. e general methods will remain e ective on more di cult face images, while many face-speci c methods will be overcome by the complications as the input becomes less constrained. us, in part this paper is a call to arms asking those interested in face stylisation to take up the challenge and try to do more.</p><p>We are hopeful that the benchmark image set will help with this endeavour. e current two levels provide a range of faces to exercise future methods, with the second level demonstrating more complications than have traditionally been a empted by many ird pair: Sisley the abstract painter <ref type="bibr" target="#b37">[Zhao and Zhu 2010]</ref>. Fourth pair: Variant of Hertzmann's layered painterly stylisation <ref type="bibr" target="#b12">[Hertzmann 1998</ref>]. Bottom pair: Circular scribble art <ref type="bibr" target="#b3">[Chiu et al. 2015]</ref>.</p><p>automated portrait stylisation techniques. Even the rst set, with its mild complications such as teeth, its varied contrast levels, and its range of face shapes, has been informative in understanding the capabilities of existing portrait methods. Further, the results we show here should be helpful to future researchers in evaluating their methods and comparing against past work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have presented two levels of a specialised benchmark to aid in evaluating portrait stylisation algorithms. e rst level corresponds to the highly constrained portrait images usually used by existing portrait-speci c stylisation techniques, with closely cropped faces in strictly frontal view, clean backgrounds, no facial hair or ornamentation, and neutral or mildly positive facial expressions. e second level relaxes the constraints on pose, lighting, and background slightly, while adding the complications of facial hair and more varied expressions. Both sets contain an even distribution of genders and a range of ethnicities. e intent is that researchers can apply their algorithms to these image sets, facilitating comparisons and ensuring neutrality in image selection. We applied six existing stylisation algorithms to the level 1 benchmark images: three existing portrait-speci c stylisation algorithms, two general-purpose stylisation algorithms, and one general learning based stylisation algorithm. e general methods coped well, but the domain knowledge from the face-speci c methods enabled them to improve the quality and robustness of their stylisation. Conversely, when we advanced to level 2, there was no change in the e ectiveness of the general methods, but the additional complications posed challenges to the face-speci c methods: in particular, facial hair sometimes confounded the algorithms. e current benchmark presented in this paper should be considered as a basic "version 0.1". Future work will look at performing user studies to con rm the separation in di culty between levels 1 and 2, and between level 2 and the proposed levels 3 and 4, which we have proposed but not constructed. e third and fourth levels will further relax the constraints on expression, background, pose, and lighting, while extending the range of subjects to include children and the elderly. Other complications reserved for yet higher levels include more elaborate poses, full-body photographs, photos of multiple people, partial occlusions, and unconstrained backgrounds.</p><p>Further future work can involve both formally extending the benchmark to even higher levels, as well as further employing the benchmark's existing two levels. We have used four portraitspeci c methods in this paper, but many more exist, and it would be of interest to document the outcomes from applying additional existing methods to the benchmark set.</p><p>Of course, future work need not be limited to work on the benchmark per se. On the contrary, the most interesting directions of future work are about portrait stylisation itself. e benchmark is intended as a tool to help with evaluation of future stylisation methods, making comparisons easier and more systematic. Finally, we hope that this paper spurs further and more ambitious work, by arguing that existing portraiture methods in NPR are highly constrained both compared to the range of photographs that people take and compared with the historical practice of portraiture. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Painted portraits: a Maori chie ain by Lindauer; a portrait of a family by Rubens; Andrea Doria as Neptune by Bronzino; Portrait of Maria Teresa de Vallabriga on horseback by Goya; Christina's World by Wyeth. All images came from Wikimedia commons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Some di cult cases for dedicated portrait stylisation methods. All images came from pexels. Photo credits:ibault Trillet, Alex Holt, Annie Spratt, Mantas Hesthaven.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Source images comprising level 1 of the portrait benchmark. All images came from Flickr unless otherwise speci ed. Photo credits: Pirátská strana, IFES -International Fellowship of Evangelical Students, Mecklenburg County, Jesse Gross (Wikimedia), iKobe, IFES -International Fellowship of Evangelical Students, Pexels (pixabay), Ethan M Sigmon, IFES -International Fellowship of Evangelical Students, projectofheart, Oregon Department of Forestry, Partij van de Arbeid, Partij van de Arbeid, chidi (pixabay), SANGONeT ICT for NGOs Conference, Mecklenburg County, jaymarable, IFES -International Fellowship of Evangelical Students, Partij van de Arbeid, Matthew Roth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Source images comprising level 2 of the portrait benchmark. All images came from Flickr unless otherwise speci ed. Photo credits: Sgt. Matthew Callahan (Wikimedia), BBC World Service, Rod Waddington, Adam McGu e, Pablo El Diablo, www.j-pics.info, susan, Nando.uy, Christopher Blizzard, Adam Jones, Christopher ompson, shankar s., ptksgc (pixabay), Sparky, Nando.uy, Martin Sharman, wiki e Photographer, Greg Peverill-Conti, Hamish Irvine, Greg Peverill-Conti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Portrait benchmark images stylised using the example-based painterly method of Wang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>Stippling<ref type="bibr" target="#b21">[Li and Mould 2011]</ref> used xed parameters for all images: exaggeration coe cients G + = 5 and G − = 5, base stipple size k = 0.1, and mask size D = 15. e method executed error di usion over a raster of height 512.• XDoG<ref type="bibr" target="#b36">[Winnemöller et al. 2012</ref>] used mostly xed parameters. However, since the XDoG contains a luminance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Portrait benchmark images stylised using the line and region method of Rosin and Lai.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Portrait benchmark images stylised using the portrait sketching method of Berger et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Portrait benchmark images stylised using the CNN MRF method of Li and Wand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Portrait benchmark images stylised using the structure-preserving stippling method ofLi and Mould.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Portrait benchmark images stylised using the XDoG method of Winnemöller et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Level 1 benchmark images processed by general NPR stylisation methods. First pair: Rosin and Lai's algorithm [Rosin and Lai 2015] without the face model. Second pair: Gastal and Oliveira's method [Gastal and Oliveira 2011].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Selected level 2 benchmark images processed by portrait stylisation methods. Top row: Rosin and Lai's method. Second row: Wang et al.'s method. ird row: Berger et al.'s method. Fourth row: Li and Wand's method. Fi h row: Li and Mould's method. Bottom row: Winnemöller et al.'s method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Design matrix for level 1.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Figure 11: Portrait benchmark image rendered in alternate styles. Top: XDoG hatching; XDoG oil paint; XDoG toon; stippling guided by ETF lines. Middle: Julian Opie variant of lines and blocks style; sketchy portrait with alternate artist; sketchy portrait with high abstraction; high abstraction and alternate artist. Bottom: CNNMRF variants: Frida Kahlo, Mona Lisa, James Hague, and mosaic styles.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>anks to the reviewers for helpful comments. Particular thanks to the various photographers, named and anonymous, who provided images. is work was supported in part by NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating open-universe face identi cation on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Analysis and Modeling of Faces and Gestures Workshop</title>
		<meeting>Analysis and Modeling of Faces and Gestures Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="904" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Style and abstraction in portrait sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Caricature generator: e dynamic exaggeration of faces by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Leonardo</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="392" to="400" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toneand Feature-Aware Circular Scribble Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Chia</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsiang</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruen-Rone</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="225" to="234" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotionally Aware Automated Portrait Painting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Colton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Digital Interactive Media in Entertainment and Arts</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal and cultural di erences in facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nebraska symposium on motivation</title>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="207" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">10 Pros and Cons Against Performance Characterization of Vision Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Förstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Performance Characteristics of Vision Algorithms</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stippling with aerial robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Galea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Kia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Aird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Kry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering</title>
		<meeting>of the Joint Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain Transform for Edge-aware Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Paint by numbers: Abstract image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Haeberli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="207" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Don&apos;t Measure -Appreciate! NPR Seen rough the Prism of Art History. In Image and Video-Based Artistic Stylisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann-Sophie</forename><surname>Lehmann</surname></persName>
		</author>
		<editor>Paul L. Rosin and John P. Collomosse</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="333" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance Characterization in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="245" to="249" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Painterly rendering with curved brush strokes of multiple sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH. ACM</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="453" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-Photorealistic Rendering and the Science of Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering</title>
		<meeting>the 8th International Symposium on Non-Photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="147" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image Analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachuse s</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Evaluating and Validating Non-photorealistic and Illustrative Rendering. In Image and Video-Based Artistic Stylisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Isenberg</surname></persName>
		</author>
		<editor>Paul L. Rosin and John P. Collomosse</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="311" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">E cient Circular resholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="992" to="1001" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A model of aesthetic appreciation and aesthetic judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Leder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Belke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="489" to="508" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Andries Oeberst, and Dorothee Augustin</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining Markov Random Fields and convolutional neural networks for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pa ern Recognition</title>
		<meeting>Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2479" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrast-aware Hal oning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="273" to="280" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structure-preserving stippling by priority-based error di usion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<publisher>Canadian Human-Computer Communications Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic classication of single facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeru</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pa ern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Authorial Subjective Evaluation of Non-photorealistic Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Non-Photorealistic Animation and Rendering (NPAR)</title>
		<meeting>the Workshop on Non-Photorealistic Animation and Rendering (NPAR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A benchmark image set for evaluating stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering</title>
		<meeting>the Joint Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image simpli cation and vectorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Non-photorealistic Animation and Rendering</title>
		<meeting>ACM Symposium on Non-photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual aesthetics and human preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">B</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sammartino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="77" to="107" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2000. e FERET evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjoon</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pa ern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-photorealistic rendering of portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Aesthetics. Eurographics Association</title>
		<meeting>the Workshop on Computational Aesthetics. Eurographics Association</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parameterisation of a stochastic model for human face identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferdinando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">C</forename><surname>Samaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Applications of Computer Vision</title>
		<meeting>Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="138" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Painting Style Transfer for Head Portraits Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face veri cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Performance characterization in computer vision: A guide to best practices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">F</forename><surname>Neil A Acker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">L</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courtney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visvanathan</forename><surname>Crum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="305" to="334" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learnable Stroke Models for Example-based Portrait Painting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darryl</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting People in Artwork with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Westlake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongping</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computer Vision for Art Analysis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AlgDesign: Algorithmic Experimental Design. R Package Version</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">XDoG: An eXtended di erence-of-Gaussians compendium including advanced image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Winnemöller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Kyprianidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><forename type="middle">C</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="740" to="753" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sisley the Abstract Painter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symp. NPAR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Abstract painting with interactive control of perceptual entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception (TAP)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

